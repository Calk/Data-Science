---> Dimensionality Reduction
It is an Unsupervised learning approach.
High dimension sets of vectors are difficult to understand
It is good for reducing computation as well. (The number of dimension for a picture is the number of pixels, for example.)
Lossy compressions involve dimensionality reduction (JPEG and MPEG for example)
Sparse points is bad because statistical testes that intend to find similarities will fail.
In a naive approach you could maintain the variables high variance and throw away the others.
Linear projections is easy to compute and analyse.
PCA gives a answer to in which axis should we project the variables. 
We are interesetd in orthogonal projection cause you loss less information in the projection.

--->PCA 
fast and well understood mathematicaly
The input of the PCA is the set of points that you have with all the fetaures.
the output of the PCA is two vectorsn where you will project your data to so dimensionality reduction.
The goal is to preserve the distances between points.
the first principal vector is the one that tries to maximizes the variance for the dataset
The second one also tries to maximize the variance but must be orthogonal to the first principal vector
If you have p features, you can generate from 1 to p principal components.
Biplot is a good way of visualizing how good the original datasets are projected on each of the vectors generated by PCA.
The pca can be understood as an aproximation hyperplane 
PCA can find the axis of the .


